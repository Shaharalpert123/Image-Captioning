# Image Captioning
## Image Captioning with Encoder-Decoder architecture on Flickr8k data set 
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/0150d496-f9e7-420b-830c-d3fbeb2f972a)


## Project for Deep Learning-046211 (Technion) Spring 2023 
## By Meir Lederman and Shahar Alpert

### in this project we will compare between two Decoder architecturs, LSTM and GRU to determine which gives better results for image captioning.

## Background
The objective of this project is to develop an image captioning system using an encoder-decoder architecture. The encoder component utilizes a pre-trained ResNet50 network, while the decoder component is LSTM or GRU networks. The training process is done on the Flickr8k dataset.

## Dataset
We used the Flickr8k dataset for the project. it consists of 8k images and 5 captions for each image. 

## Models
For our Encoder we used a pretrained CNN â€“ ResNet50 which was trained on the ImageNet dataset so it can extract features out of images very efficiently. 
For the Decoder we analyzed two different architectures: LSTM and GRU both with the goal to create a caption for the image using the features vector from the encoder.

## requierments 
Full lists of requirements are in the requirements.txt file. To install the requirements run: pip install -r requirements.txt

## running the project:
the code is in one ipynb notebook, after installing the requirements run it cell by cell. the project is meant to run on google colab (because the name of the directories) and will maybe require minor changes to run on different environments.  

# Results
## LSTM
### loss graph:
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/a060a08e-bffc-43a3-b337-db352aebca8e)

### captions generated by the LSTM model:
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/3d3f9c68-8607-4eb3-87a9-5c3695fdba0e)
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/0df3a3ed-52fd-49ed-b790-745946df892c)
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/73cd5d1b-04e0-418e-b104-cbaa0142ff94)
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/97111d36-158e-4f04-89e1-57a20e97feff)


## GRU 
### loss graph:
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/e49e729b-2e83-45db-9b58-8e32db44c73c)

### captions generated by the GRU model:
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/9de01bb9-53be-45a3-b333-1f985acfdf4d)
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/8acde6f8-bb90-49c3-9e44-15beeff2b4c3)
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/2d47542f-e41d-4653-bb46-98b09cd81993)
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/011d9aa6-f526-4c33-82d3-9a73e870a0ad)

we can see that in both models some captions are good and some captions are very bad. most of the time the model has a reasonable idea of what happening in the image but not precise on the details.  

## References
We used the structure of the code of https://github.com/luizwainstein/046211_Image_Captioning

