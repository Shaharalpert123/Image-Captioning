# Image Captioning
## Image Captioning with Encoder-Decoder architecture on Flickr8k data set 
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/0150d496-f9e7-420b-830c-d3fbeb2f972a)

## Project for Deep Learning-046211 (Technion) Spring 2023 
## By Meir Lederman and Shahar Alpert

### in this project we will compare between two Decoder architecturs, LSTM and GRU to determine which gives better results for image captioning.

## Background
The objective of this project is to develop an image captioning system using an encoder-decoder architecture. The encoder component utilizes a pre-trained ResNet50 network, while the decoder component is LSTM or GRU networks. The training process is done on the Flickr8k dataset.

## Dataset
We used the Flickr8k dataset for the project. it consists of 8k images and 5 captions for each image. 

## Models
For our Encoder we used a pretrained CNN â€“ ResNet50 which was trained on the ImageNet dataset so it can extract features out of images very efficiently. 
For the Decoder we analyzed two different architectures: LSTM and GRU both with the goal to create a caption for the image using the features vector from the encoder.

# Results
## GRU 
### loss graph:
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/e49e729b-2e83-45db-9b58-8e32db44c73c)

### captions generated by the GRU model:
![image](https://github.com/Shaharalpert123/Image-Captioning/assets/139067940/9de01bb9-53be-45a3-b333-1f985acfdf4d)
